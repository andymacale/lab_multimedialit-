I video non compressi contengono una quantità di dati veramente immensa. Ad esempio, per elaborare un segnale video HDTV-R a 720p a $60 \, \text{fps}$:
\begin{equation*}
    c = \left(720 \times 1280 \, \frac{\text{px}}{\text{frame}}\right) \times \left(60 \, \frac{\text{frame}}{\text{s}}\right) \times
    \left(3 \,  \frac{\text{color}}{\text{px}}\right) \times \left(8 \,  \frac{\text{b}}{\text{color}}\right) =
    1,3264 \, \text{Gb/s}
\end{equation*}
dove la banda del canale è solamente $20 \, \text{Mb/s}$: da qui si rende necessario comprimere i video.
\section{Codifica dei video}
La codifica video consiste nel ridurre la ridondanza spaziale con i frame, similmente a come avviene in JPEG, e temporale.
\subsection{Differenza tra frame}
Un video non è altro che una sequenza di frame, in cui gli oggetti appiaiono, si muovono e scompaiono e i pixel di sfondo rimangono gli stessi. A
questo punto, se si sottraggono due immagini:
\begin{itemize}
    \item il cambiamento di sfondo è solamente rumore;
    \item i bordi degli oggetti presentano dei cambiamenti significativi.
\end{itemize}
Questa operazione prende il nome di DPCM (Difference Pulse-Code Modulation), in cui il frame 0 è il fermo immagine ed il resto sono la differenza
tra il frame corrente e quello precedente: per esempio la differenza frame 1 è la differenza tra il frame 1 ed il frame 0, la differenza frame 2 è la 
differenza tra il frame 2 ed il frame 1 e così via. Perciò:
\begin{itemize}
    \item se la scena è priva di movimento, allora tutte le differenze di frame sono nulle, potendo comprimere molto contenuto informativo;
    \item se nella scena è presente movimento, si può vedere la differenza tra le due immagini.
\end{itemize}
La differenza tra due frame può essere causata:
\begin{itemize}
    \item il movimento della camera di contorni sullo sfondo o di oggetti fissi si possono vedere dalla immagine differenziale;
    \item il movimento degli oggetti si può vedere grazie ai contorni degli oggetti in movimento;
    \item i cambi di illuminazione, come fari e lampioni;
    \item tagli di sceni, l'immagine differenziale presenti dei cambiamenti netti;
    \item rumore.
\end{itemize}
A questo punto, se la differenza tra due frame è solamente rumore, allora si preferisce avere differenza nulla. Invece, se si riesce a vedere qualcosa
nell'immagine differenziale ed anche a riconoscerla, significa che è presente correlazione nell'immagine differenziale. L'obiettivo è rimuovere la
correlazione, compensando il movimento.
\begin{center}
\begin{minipage}{0.30\textwidth} % Colonna Sinistra (48% della larghezza del testo)
    \centering
        \includegraphics[width=0.9\textwidth]{cap10/f0}     
\end{minipage}
\hfill % Spazio elastico per separare le due colonne
\begin{minipage}{0.30\textwidth} % Colonna Destra (48% della larghezza del testo)
    \centering
        \includegraphics[width=0.9\textwidth]{cap10/f1} 
\end{minipage}
\hfill % Spazio elastico per separare le due colonne
\begin{minipage}{0.30\textwidth} % Colonna Destra (48% della larghezza del testo)
    \centering
        \includegraphics[width=0.9\textwidth]{cap10/fd} 
\end{minipage}
\captionof{figure}{Differenza tra $f_0$ (a sinistra) a $f_1$ (al centro) mostrato a destra}
\label{fig:euhfeihfeihfeifhieeihei}
\end{center}
Nella codifica video, si definiscono due tipologie di frame:
\begin{itemize}
    \item key frame, se la compressione è basata sul contenuto del frame;
    \item delta frame, se la compressione è basata sul contenuto dell'ultimo key frame.
\end{itemize}
\begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{cap10/frame} 
        \caption{Sequenza di key frame e delta frame} 
        \label{fig:seq_key_delta}
\end{figure}
Infine, la quantità di dati da codificare può essere ridotto significativamente se il frame precedente è sottratto dal frame corrente: 
in questo modo si ottiene la cosiddetta immagine residua.
\subsection{Motion JPEG e sfruttamento della ridondanza temporale}
Nella fase di Motion JPEG, avviene la compressione JPEG ad ogni fotogramma del video, effettuando una compressione puramente spaziale con velocità di
compressione da 2:1 a 12:1 con perdita di dati fino a 5:1, detta qualità broadcast. A ciò si aggiunge lo sfruttamento della ridondanza temporale, infatti
esistono tre tipologie di movimento:
\begin{itemize}
    \item traslazione nella fase, il movimento tipico degli oggetti rigidi;
    \item rotazione attraverso un asse;
    \item zoom.
\end{itemize}
Ciò, permette di comprendere i parametri che descrivono il movimento. In particolare, si prendono alcune porzioni di frame e si stima il moto tra due
frame: il frame corrente ed il frame di riferimento. Il problema è capire bene quali sono le porzioni.\\
Infine, si esegue la codifica ed avviene attraverso i vettori di movimento $dx$ e $dy$. L'idea generale sta nel trovare una regione $P_C$ nel frame
corrente, trovare una regione $P_R$ nella finestra di ricerca nel frame di riferimento, per minimizzare l'errore tra la regione $P_R$ e $P_C$.
\begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{cap10/regioni} 
        \caption{Stima del movimento} 
        \label{fig:regioni}
\end{figure}
\subsection{Block matching}
A questo punto, gli algoritmi di codifica dei video solitamente contengono due tipologie di schemi di codifica:
\begin{itemize}
    \item la codifca intraframe, che è simile alla codifica di immagini fisse, poiché non sfrutta la correlazione di frame adiacenti;
    \item la codifica interframe, che invece include la stima e la componensazione del movimento, per ridurre la ridondanza temporale.
\end{itemize}
La tecnica del block matching consiste nel dividere il frame corrente in blocchi, per trovare un blocco candidato nella regione di ricerca, che ha la
correlazione maggiore con il blocco sorgente. In particolare, la differenza tra il blocco sorgente ed il blocco candidato prende il nome di vettori di
movimento. Infatti, quest'ultimo descrive la distanza tra la posizione del blocco in fase di codifica e la posizione del blocco con la corrispondenza 
migliore nel frame di riferimento. Infine, l'immagine residua dell'errore contiene le differenze tra l'immagine desiderata e quella prevista. Ciò, si
calcola con l'MSE o l'MAE.
\begin{center}
\begin{minipage}{0.48\textwidth} % Colonna Sinistra (48% della larghezza del testo)
    \centering
        \begin{equation*}
            MSE = \sum\left[ b\left(B_{ref}\right) - b(\left(B_{curr}\right) \right]^2
        \end{equation*}     
\end{minipage}
\hfill % Spazio elastico per separare le due colonne
\begin{minipage}{0.48\textwidth} % Colonna Destra (48% della larghezza del testo)
    \begin{equation*}
            MAE = \sum\left| b\left(B_{ref}\right) - b(\left(B_{curr}\right)\right|
    \end{equation*}
\end{minipage}
\end{center}
\section{Processo di compressione video}
In questo paragrafo si entra nel cuore del capitolo: vengono spiegate le tecniche della compressione video.
\subsection{Compressione basata sul block matching}
La compressione basata sul block matching csi basa sulla tecnica appena spiegata del block matching. Assumendo un frame $f_{-1}$ che è stato codificato
e ricostruito, ed il frame corrente che deve essere codificato $f$. A questo punto si procede nella maniera seguente:
\begin{enumerate}
    \item si divide $f$ in blocchi della stessa dimensione;
    \item per ogni blocco, si trova il vettore di movimento usando il block matching, che si basa su $f_{-1}$, e si esegue la display frame difference;
    \item si trasmette il vettore di movimento per ogni blocco al decoder;
    \item si comprime ogni blocco a cui è stata applicata la display frame difference;
    \item si trasmette la display frame difference al decoder.
\end{enumerate}
\subsection{Compressione basata sul codificatore ibrido}
Il codificatore ibrido combina la stima del moto con la trasformata spaziale. Ciò consiste in:
\begin{enumerate}
    \item si stima il moto e si ottengono i vettori di moto;
    \item si calcola l'immagine di residuo tra il blocco originale e quello predetto;
    \item all'immagine residuo, si applica la DCT, la quantizzazione e la codifica VLC;
    \item si trasmettono i vettori di moto ed il residuo codificato;
    \item si esegue il cosiddetto loop di feedback, in cui il codificatore simula il decoder per assicurarsi che le predizioni si basano sugli stessi
            dati che avrà il ricevente.
\end{enumerate}
Di seguito, ne viene riportato uno schema.
\begin{figure}[htbp]
        \centering
        \includegraphics[width=0.9\textwidth]{cap10/schema} 
        \caption{Schema di compressione video basata sul codificatore ibrido} 
        \label{fig:schema_compressione_video}
\end{figure}
\section{Standard MPEG}
In questo paragrafo viene spiegato come avviene la compressione video con lo standard MPEG e le sue varie versioni.
\subsection{Compressione video MPEG}
Lo standard MPEG classifica tutti i frame di un video in tre gruppi:
\begin{itemize}
    \item intra-frame, abbreviati in I-frame, che sono codificati senza riferirsi ad altri frame, necessari per ridurre l'errore di propagazione, per
            permettere l'accesso randomico e per il taglio attraverso le scene;
    \item predictive-frame, indicati con P-frame, sono codificati con riferimento al frame precedente, permettendo un tasso di compressione medio;
    \item bidirectional-frame, detti anche B-frame, vengono codificati con riferimento al frame precedente ed al frame successivo, avendo un tasso di
            compressione alto, aumentando però la complessità computazionale.
\end{itemize}
\begin{figure}[htbp]
        \centering
        \includegraphics[width=0.9\textwidth]{cap10/rappresentazione} 
        \caption{Rappresentazione da sinistra rispettivamente degli I-frame, P-frame e B-frame} 
        \label{fig:schema_frame_MPEG}
\end{figure}
A questo punto, le tre tipologie di frame vengono organizzate in GOP (Group of Pictures), con un ordine ben preciso, poiché i B-frame richiedono che I
frame futuri siano già stati elaborati. In questo modo, la ridondanza temporale viene descritta dal movimento degli oggetti attraverso i frame dagli
I-frame ai P-frame ed i B-frame. Infatti, i P-frame ed i B-frame sono codificati tramite i vettori di movimento ed il corrispettivo errore di predizione.
\begin{figure}[htbp]
        \centering
        \includegraphics[width=0.9\textwidth]{cap10/gop} 
        \caption{Rappresentazione del GOP} 
        \label{fig:gop}
\end{figure}
\subsection{Versioni di MPEG}
Di seguito viene riportata l'evoluzione di MPEG, con i vari miglioramenti:
\begin{itemize}
    \item MPEG-1: supporta solo la scansione progressiva, perciò bassa qualità e non adatto ai broadcast TV;
    \item MPEG-2: introduce il supporto video interlacciato e l'alta risoluzione TV, però con file pesanti ed alto utilizzo della CPU;
    \item MPEG-3: abbandonato, perché MPEG-2 è più che sufficiente per l'HDTV;
    \item MPEG-4: si basa sui video-object (VO), permettendo interattività, gestione contenuti 2D e 3D ed è scalabile da bit-rate molto bassi a molto alti;
    \item H.264/AVC: migliore efficienza di compressione, poiché usa blocchi più piccoli e trasformate intere ed un filtro di deblocking integrato per migliorare la qualità visiva;
    \item HEVC (H.265): progettato per risoluzioni in 4K, per ridurre il bit-rate anche del $50 \%$, usa una struttura a Quadtree con Coding Tree Unit (CTU) da blocchi 8x8 fino a 64x64, che si adattano meglio al contenuto dell'immagine.
\end{itemize}
\begin{figure}[htbp]
        \centering
        \includegraphics[width=0.5\textwidth]{cap10/ver} 
        \caption{Differenza tra HEVC e H.264/AVC} 
        \label{fig:ver}
\end{figure}
\vfill